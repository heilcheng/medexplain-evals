# MedExplain-Evals

[![Google Summer of Code](https://img.shields.io/badge/Google%20Summer%20of%20Code-2025-4285F4?style=flat&logo=google&logoColor=white)](https://summerofcode.withgoogle.com/)
[![Google DeepMind](https://img.shields.io/badge/Google%20DeepMind-4285F4?style=flat&logo=google&logoColor=white)](https://deepmind.google/)
[![Documentation](https://img.shields.io/badge/docs-GitHub%20Pages-blue)](https://heilcheng.github.io/medexplain-evals/)

> **This project is developed as part of [Google Summer of Code 2025](https://summerofcode.withgoogle.com/) with [Google DeepMind](https://deepmind.google/).**

A benchmark for evaluating audience-adaptive medical explanation quality in large language models.

## Overview

MedExplain-Evals provides infrastructure for:

- Evaluating how well LLMs generate medical explanations for different audiences (physicians, nurses, patients, caregivers)
- Measuring explanation quality across six dimensions (accuracy, terminology, completeness, actionability, safety, empathy)
- Using ensemble LLM-as-Judge with weighted scoring from multiple frontier models
- Grounding evaluations against medical knowledge bases (UMLS, RxNorm, SNOMED-CT)
- Generating publication-ready analysis reports and visualizations

ğŸ“– **[Read the full documentation â†’](https://heilcheng.github.io/medexplain-evals/)**

## Requirements

### Software

- Python 3.10+
- API keys for at least one LLM provider (OpenAI, Anthropic, Google)
- 10 GB+ disk space (datasets and results)

### Hardware (for local models)

| Model Size | Minimum VRAM | Recommended | With Quantization |
|------------|--------------|-------------|-------------------|
| 7-9B       | 8 GB         | 16 GB       | 5 GB              |
| 13-14B     | 16 GB        | 24 GB       | 8 GB              |
| 70B+       | 40 GB        | 80 GB+      | 20 GB             |

## Installation

```bash
git clone https://github.com/heilcheng/medexplain-evals.git
cd medexplain-evals

python -m venv venv
source venv/bin/activate

pip install -r requirements.txt
```

## Benchmark Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        MedExplain-Evals Pipeline                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚   Input     â”‚    â”‚   LLM Under     â”‚    â”‚     Generated Explanation   â”‚â”‚
â”‚  â”‚  Clinical   â”‚â”€â”€â”€â–¶â”‚     Test        â”‚â”€â”€â”€â–¶â”‚     for Target Audience     â”‚â”‚
â”‚  â”‚  Scenario   â”‚    â”‚  (API/Local)    â”‚    â”‚                             â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                            â”‚                â”‚
â”‚                                                            â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    Evaluation Engine                                    â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚â”‚
â”‚  â”‚  â”‚ Knowledge      â”‚  â”‚  Ensemble      â”‚  â”‚     Safety                 â”‚â”‚â”‚
â”‚  â”‚  â”‚ Grounding      â”‚  â”‚  LLM-as-Judge  â”‚  â”‚     Evaluator              â”‚â”‚â”‚
â”‚  â”‚  â”‚                â”‚  â”‚                â”‚  â”‚                            â”‚â”‚â”‚
â”‚  â”‚  â”‚ â€¢ UMLS Lookup  â”‚  â”‚ â€¢ GPT-5.x      â”‚  â”‚ â€¢ Drug interactions       â”‚â”‚â”‚
â”‚  â”‚  â”‚ â€¢ RxNorm Match â”‚  â”‚ â€¢ Claude 4.5   â”‚  â”‚ â€¢ Contraindications       â”‚â”‚â”‚
â”‚  â”‚  â”‚ â€¢ SNOMED-CT    â”‚  â”‚ â€¢ Gemini 3     â”‚  â”‚ â€¢ Harm classification     â”‚â”‚â”‚
â”‚  â”‚  â”‚ â€¢ NLI Verify   â”‚  â”‚ â€¢ DeepSeek-V3  â”‚  â”‚ â€¢ Warning detection       â”‚â”‚â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚â”‚
â”‚  â”‚                                                                         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                            â”‚                â”‚
â”‚                                                            â–¼                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚                    Dimension Scoring (Weighted)                         â”‚â”‚
â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚
â”‚  â”‚  Factual Accuracy (25%) â”‚ Terminology (15%) â”‚ Completeness (20%)       â”‚â”‚
â”‚  â”‚  Actionability (15%)    â”‚ Safety (15%)      â”‚ Empathy & Tone (10%)     â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â”‚                                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Medical Knowledge Grounding

MedExplain-Evals grounds factual claims against established medical ontologies:

### UMLS (Unified Medical Language System)

The UMLS integrates over 200 source vocabularies and provides a comprehensive framework for medical concept normalization.

| Resource | Description | Usage in MedExplain |
|----------|-------------|---------------------|
| Metathesaurus | Unified concepts from 200+ sources | Entity linking, concept validation |
| Semantic Network | Semantic types and relations | Relationship verification |
| SPECIALIST Lexicon | Biomedical language tools | Term normalization |

**Citation:**
```bibtex
@article{bodenreider2004umls,
  author = {Bodenreider, Olivier},
  title = {The Unified Medical Language System (UMLS): integrating biomedical terminology},
  journal = {Nucleic Acids Research},
  volume = {32},
  number = {suppl\_1},
  pages = {D267--D270},
  year = {2004},
  doi = {10.1093/nar/gkh061},
  url = {https://www.nlm.nih.gov/research/umls/}
}
```

### RxNorm

RxNorm provides normalized names and identifiers for clinical drugs, enabling drug safety verification.

| Component | Description | Usage in MedExplain |
|-----------|-------------|---------------------|
| RxCUI | Unique drug identifiers | Drug entity validation |
| Drug classes | Therapeutic categories | Drug-condition matching |
| Ingredient links | Active ingredient mapping | Interaction checking |

**Citation:**
```bibtex
@article{nelson2011rxnorm,
  author = {Nelson, Stuart J and Zeng, Kelly and Kilbourne, John and Powell, Tammy and Moore, Robin},
  title = {Normalized names for clinical drugs: RxNorm at 6 years},
  journal = {Journal of the American Medical Informatics Association},
  volume = {18},
  number = {4},
  pages = {441--448},
  year = {2011},
  doi = {10.1136/amiajnl-2011-000116},
  url = {https://www.nlm.nih.gov/research/umls/rxnorm/}
}
```

### SNOMED CT (Clinical Terms)

SNOMED CT is the most comprehensive clinical terminology, covering diseases, procedures, and clinical findings.

| Hierarchy | Description | Usage in MedExplain |
|-----------|-------------|---------------------|
| Clinical finding | Disorders, symptoms | Diagnosis validation |
| Procedure | Medical interventions | Treatment verification |
| Body structure | Anatomical concepts | Anatomical accuracy |
| Pharmaceutical product | Medications | Drug reference checking |

**Citation:**
```bibtex
@article{donnelly2006snomed,
  author = {Donnelly, Kevin},
  title = {SNOMED-CT: The advanced terminology and coding system for eHealth},
  journal = {Studies in Health Technology and Informatics},
  volume = {121},
  pages = {279--290},
  year = {2006},
  url = {https://www.snomed.org/}
}
```

### Additional Medical Resources

| Resource | Purpose | Citation |
|----------|---------|----------|
| **MedDRA** | Adverse event terminology | [ICH MedDRA](https://www.meddra.org/) |
| **ICD-10/11** | Disease classification | [WHO ICD](https://www.who.int/standards/classifications/icd) |
| **LOINC** | Lab/clinical observations | [Regenstrief LOINC](https://loinc.org/) |
| **DrugBank** | Drug interaction data | [DrugBank 5.0](https://go.drugbank.com/) |

## Supported Models

| Provider   | Models                                        | Multimodal |
|------------|-----------------------------------------------|------------|
| OpenAI     | GPT-5.2, GPT-5.1, GPT-5, GPT-4o              | âœ“          |
| Anthropic  | Claude Opus 4.5, Sonnet 4.5, Haiku 4.5       | âœ“          |
| Google     | Gemini 3 Pro, Flash                          | âœ“          |
| Meta       | Llama 4 Behemoth, Maverick, Scout            | âœ“          |
| DeepSeek   | DeepSeek-V3                                  |            |
| Alibaba    | Qwen3-Max, Qwen3 family                      | âœ“          |
| Amazon     | Nova Pro, Nova Omni                          | âœ“          |

## Evaluation Dimensions

| Dimension                      | Weight | Description                                      |
|--------------------------------|--------|--------------------------------------------------|
| Factual Accuracy               | 25%    | Clinical correctness and evidence alignment      |
| Terminological Appropriateness | 15%    | Language complexity matching audience needs      |
| Explanatory Completeness       | 20%    | Comprehensive yet accessible coverage            |
| Actionability                  | 15%    | Clear, practical guidance                        |
| Safety                         | 15%    | Appropriate warnings and harm avoidance          |
| Empathy & Tone                 | 10%    | Audience-appropriate communication style         |

## Target Audiences

| Audience   | Variants                          | Health Literacy     |
|------------|-----------------------------------|---------------------|
| Physicians | Specialist, Generalist            | Expert              |
| Nurses     | ICU, General Ward, Specialty      | Professional        |
| Patients   | Low, Medium, High literacy        | Variable            |
| Caregivers | Family, Professional, Pediatric   | Variable            |

## Quick Start

```bash
# Set API keys
export OPENAI_API_KEY=your_key
export ANTHROPIC_API_KEY=your_key

# Validate environment
python scripts/validate_environment.py

# Run evaluation
python scripts/run_evaluation.py \
  --config configs/evaluation_config.yaml \
  --models gpt-5.1 claude-opus-4.5 \
  --audiences patient_low_literacy physician_specialist \
  --output results/
```

## Configuration

```yaml
models:
  gpt-5.1:
    provider: openai
    temperature: 0.3
    max_tokens: 2048

  claude-opus-4.5:
    provider: anthropic
    temperature: 0.3

audiences:
  - patient_low_literacy
  - patient_medium_literacy
  - physician_specialist
  - nurse_general

evaluation:
  dimensions:
    factual_accuracy: 0.25
    terminological_appropriateness: 0.15
    explanatory_completeness: 0.20
    actionability: 0.15
    safety: 0.15
    empathy_tone: 0.10

judge:
  ensemble:
    - model: gpt-5.1
      weight: 0.30
    - model: claude-opus-4.5
      weight: 0.30
    - model: gemini-3-pro
      weight: 0.25
    - model: deepseek-v3
      weight: 0.15

output:
  path: ./results
  formats: [json, csv, html]
```

## Python API

```python
from src import UnifiedModelClient, EnsembleLLMJudge, PersonaFactory

# Initialize
client = UnifiedModelClient()
judge = EnsembleLLMJudge(client)
persona = PersonaFactory.get_predefined_persona("patient_low_literacy")

# Generate explanation
explanation = client.generate(
    model="gpt-5.1",
    messages=[{"role": "user", "content": "Explain type 2 diabetes simply"}]
)

# Evaluate
score = judge.evaluate(
    original="Type 2 diabetes mellitus with HbA1c 8.5%...",
    explanation=explanation.content,
    audience=persona
)

print(f"Overall: {score.overall:.2f}/5.0")
print(f"Agreement: {score.agreement_score:.2f}")
```

## Web Platform

An interactive web interface is available for browser-based evaluation:

```bash
# Backend
cd web/backend
pip install -r requirements.txt
uvicorn app.main:app --port 8000

# Frontend
cd web/frontend
npm install && npm run dev
```

Access at http://localhost:3000

Features:
- **Dashboard**: Overview of evaluation runs and statistics
- **Playground**: Interactive testing of medical explanations
- **Models**: Configure API and local model providers
- **Audiences**: Browse 11 medical audience personas
- **Results**: Visualize evaluation scores and rankings

## Output Structure

```
results/
â”œâ”€â”€ 20251229_143022/
â”‚   â”œâ”€â”€ scores.json
â”‚   â”œâ”€â”€ explanations/
â”‚   â”‚   â”œâ”€â”€ gpt-5.1/
â”‚   â”‚   â””â”€â”€ claude-opus-4.5/
â”‚   â”œâ”€â”€ analysis/
â”‚   â”‚   â”œâ”€â”€ dimension_breakdown.png
â”‚   â”‚   â”œâ”€â”€ audience_comparison.png
â”‚   â”‚   â””â”€â”€ model_rankings.png
â”‚   â””â”€â”€ report.html
```

## Testing

```bash
pytest tests/
pytest --cov=src tests/
```

## Project Structure

```
medexplain-evals/
â”œâ”€â”€ src/                       # Core Python library
â”‚   â”œâ”€â”€ clients/               # LLM API clients
â”‚   â”œâ”€â”€ evaluation/            # Scoring and judging
â”‚   â”œâ”€â”€ personas/              # Audience modeling
â”‚   â”œâ”€â”€ data/                  # Data loading
â”‚   â”œâ”€â”€ knowledge/             # UMLS/RxNorm grounding
â”‚   â”œâ”€â”€ core/                  # Shared utilities
â”‚   â””â”€â”€ rubrics/               # G-Eval scoring rubrics
â”œâ”€â”€ scripts/                   # CLI tools
â”œâ”€â”€ web/                       # Web platform (Next.js + FastAPI)
â”œâ”€â”€ docs/                      # Sphinx documentation
â”œâ”€â”€ analysis/                  # Visualization and reporting
â”œâ”€â”€ configs/                   # Configuration templates
â”œâ”€â”€ data/                      # Sample datasets
â”œâ”€â”€ tests/                     # Test suite
â””â”€â”€ examples/                  # Usage examples
```

## Documentation

Full documentation is available at: https://heilcheng.github.io/medexplain-evals/

## Citation

If you use MedExplain-Evals in your research, please cite:

```bibtex
@software{medexplain2025,
  author = {Hailey Cheng},
  title = {MedExplain-Evals: A Benchmark for Audience-Adaptive Medical Explanation Quality in LLMs},
  year = {2025},
  url = {https://github.com/heilcheng/medexplain-evals}
}
```

## License

MIT License. See [LICENSE](LICENSE) for details.

## Disclaimer

MedExplain-Evals is designed for research evaluation purposes only. Generated explanations should not be used for actual medical advice. Always consult qualified healthcare professionals for medical decisions.
